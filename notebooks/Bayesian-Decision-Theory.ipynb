{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "309de221",
   "metadata": {},
   "source": [
    "\n",
    "# Bayesian Decision Theory\n",
    "\n",
    "Chapter 2 of Pattern Classification {cite}`Duda2000`\n",
    "\n",
    "![](img/Duda-front.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3384dc2b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- **Bayesian Decision Theory** is a statistical approach to pattern classification.\n",
    "- It quantifies tradeoffs between classification decisions using probabilities and costs.\n",
    "- Assumes all relevant probabilities are known.\n",
    "- Example: Classifying fish (sea bass vs. salmon) based on features like lightness.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "- **State of Nature** ($ \\omega $): Represents the true category (e.g., sea bass or salmon).\n",
    "- **Prior Probability** ($ P(\\omega_j) $): Probability of a category before observing any data.\n",
    "- **Class-Conditional Probability Density** ($ p(x|\\omega_j) $): Probability of observing feature $ x $ given category $ \\omega_j $.\n",
    "- **Posterior Probability** ($ P(\\omega_j|x) $): Probability of category $ \\omega_j $ after observing feature $ x $.\n",
    "\n",
    "---\n",
    "\n",
    "## Bayes' Formula\n",
    "- Bayes' Theorem:\n",
    "\n",
    "  $$\n",
    "  P(\\omega_j|x) = \\frac{p(x|\\omega_j) P(\\omega_j)}{p(x)}\n",
    "  $$\n",
    "\n",
    "  - **Posterior** = (Likelihood × Prior) / Evidence\n",
    "- **Evidence** ($ p(x) $): Normalizing constant, often ignored in classification.\n",
    "\n",
    "  $$\n",
    "  p(x) = \\sum_{j=1}^c p(x|\\omega_j) P(\\omega_j)\n",
    "  $$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b87d73",
   "metadata": {},
   "source": [
    "\n",
    "The probability $ P(\\omega_j) $ is converted to the **a posteriori probability** (or **posterior probability**) $ P(\\omega_j|x) $ — the probability of the state of nature being $ \\omega_j $ given that feature value $ x $ has been measured. We call $ p(x|\\omega_j) $ the **likelihood** of $ \\omega_j $ with respect to $ x $ (a term chosen to indicate that, other things being equal, the category $ \\omega_j $ for which $ p(x|\\omega_j) $ is large is more \"likely\" to be the true category). \n",
    "\n",
    "Notice that it is the product of the **likelihood** and the **prior probability** that is most important in determining the posterior probability; the **evidence factor**, $ p(x) $, can be viewed as merely a scale factor that guarantees that the posterior probabilities sum to one, as all good probabilities must. The variation of $ P(\\omega_j|x) $ with $ x $ is illustrated in **Figure 2.2** for the case $ P(\\omega_1) = \\frac{2}{3} $ and $ P(\\omega_2) = \\frac{1}{3} $.\n",
    "\n",
    "\n",
    "![Figure 2.1](img/Duda-Figure-2.1.png)\n",
    "\n",
    "**Figure 2.1**: Hypothetical class-conditional probability density functions show the probability density of measuring a particular feature value $ x $ given the pattern is in category $ \\omega_i $. If $ x $ represents the length of a fish, the two curves might describe the difference in length of populations of two types of fish. Density functions are normalized, and thus the area under each curve is 1.0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5a0cb0",
   "metadata": {},
   "source": [
    "\n",
    "## Decision Rule\n",
    "- **Minimum Error Rate Classification**:\n",
    "  - Decide $ \\omega_1 $ if $ P(\\omega_1|x) > P(\\omega_2|x) $, otherwise decide $ \\omega_2 $.\n",
    "  - Equivalent to:\n",
    "    $\n",
    "    \\text{Decide } \\omega_1 \\text{ if } p(x|\\omega_1) P(\\omega_1) > p(x|\\omega_2) P(\\omega_2)\n",
    "    $\n",
    "  - Probability of error:\n",
    "    $\n",
    "    P(\\text{error}|x) = \\min[P(\\omega_1|x), P(\\omega_2|x)]\n",
    "    $\n",
    "\n",
    "\n",
    "![Figure 2.2](img/Duda-Figure-2.2.png)\n",
    "\n",
    "**Figure 2.2**: Posterior probabilities for the particular priors $P(\\omega_1) = \\frac{2}{3}$ and $P(\\omega_2) = \\frac{1}{3}$ for the class-conditional probability densities shown in **Figure 2.1**. Thus, in this case, given that a pattern is measured to have feature value $x = 14$, the probability it is in category $\\omega_2$ is roughly $0.08$, and that it is in $\\omega_1$ is $0.92$. At every $x$, the posteriors sum to $1.0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da92564",
   "metadata": {},
   "source": [
    "Some additional insight can be obtained by considering a few special cases:\n",
    "\n",
    "1. **Case 1**: If for some $ x $ we have $ p(x|\\omega_1) = p(x|\\omega_2) $, then that particular observation gives us no information about the state of nature. In this case, the decision hinges entirely on the **prior probabilities** $ P(\\omega_1) $ and $ P(\\omega_2) $.\n",
    "\n",
    "2. **Case 2**: If $ P(\\omega_1) = P(\\omega_2) $, then the states of nature are equally probable. In this case, the decision is based entirely on the **likelihoods** $ p(x|\\omega_j) $.\n",
    "\n",
    "3. **General Case**: In general, both the **prior probabilities** and the **likelihoods** are important in making a decision. The **Bayes decision rule** combines these factors to achieve the **minimum probability of error**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f66d63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Generalization to More Than Two Classes\n",
    "\n",
    "The **Bayes decision rule** to minimize risk calls for selecting the action that minimizes the **conditional risk**. To minimize the average probability of error, we should select the class $i$ that maximizes the **posterior probability** $P(\\omega_i|\\mathbf{x})$. In other words, for **minimum error rate**:\n",
    "\n",
    "$$\n",
    "\\text{Decide } \\omega_i \\text{ if } P(\\omega_i|\\mathbf{x}) > P(\\omega_j|\\mathbf{x}) \\quad \\text{for all } j \\neq i.\n",
    "$$\n",
    "\n",
    "This rule generalizes naturally to **multiple classes** ($c > 2$). For each class $\\omega_i$, we compute the posterior probability $P(\\omega_i|\\mathbf{x})$ and assign the feature vector $\\mathbf{x}$ to the class with the highest posterior probability. This ensures that the **probability of error** is minimized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82d1e8e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Discriminant Functions\n",
    "- **Discriminant Function** ($ g_i(\\mathbf{x}) $): Used to assign a feature vector $ \\mathbf{x} $ to class $ \\omega_i $.\n",
    "  - For minimum error rate:\n",
    "    $\n",
    "    g_i(\\mathbf{x}) = P(\\omega_i|\\mathbf{x})\n",
    "    $\n",
    "  - Can also be expressed as:\n",
    "    $\n",
    "    g_i(\\mathbf{x}) = p(\\mathbf{x}|\\omega_i) P(\\omega_i)\n",
    "    $\n",
    "  - Or in log form:\n",
    "    $\n",
    "    g_i(\\mathbf{x}) = \\ln p(\\mathbf{x}|\\omega_i) + \\ln P(\\omega_i)\n",
    "    $\n",
    "\n",
    "\n",
    "![](img/Duda-Figure-2.5.png)\n",
    "Figure 2.5: The functional structure of a general statistical pattern classifier which includes $d$ inputs and $c$ discriminant functions $g_i(\\mathbf{x})$. A subsequent step determines which of the discriminant values is the maximum, and categorizes the input pattern accordingly. The arrows show the direction of the flow of information, though frequently the arrows are omitted when the direction of flow is self-evident."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba74031",
   "metadata": {},
   "source": [
    "![Figure 2.6](img/Duda-Figure-2.6.png)\n",
    "\n",
    "**Figure 2.6**: In this two-dimensional two-category classifier, the probability densities are Gaussian (with $1/e$ ellipses shown), the decision boundary consists of two hyperbolas, and thus the decision region $\\mathcal{R}_2$ is not simply connected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0241d14",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Normal Density\n",
    "- **Univariate Normal Density**:\n",
    "  $\n",
    "  p(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right]\n",
    "  $\n",
    "\n",
    "\n",
    "For which the **expected value** of $x$ (an average, here taken over the feature space) is:\n",
    "\n",
    "$$\n",
    "\\mu = \\mathbb{E}[x] = \\int_{-\\infty}^{\\infty} x \\, p(x) \\, dx \n",
    "$$\n",
    "\n",
    "and where the **expected squared deviation** or **variance** is:\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\mathbb{E}[(x - \\mu)^2] = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 \\, p(x) \\, dx \n",
    "$$\n",
    "\n",
    "The **univariate normal density** is completely specified by two parameters: its **mean** $\\mu$ and **variance** $\\sigma^2$. For simplicity, we often abbreviate $p(x)$ by writing $p(x) \\sim N(\\mu, \\sigma^2)$ to say that $x$ is distributed normally with mean $\\mu$ and variance $\\sigma^2$. Samples from normal distributions tend to cluster about the mean, with a spread related to the **standard deviation** $\\sigma$ (see **Figure 2.7**).\n",
    "\n",
    "\n",
    "![Figure 2.7](img/Duda-Figure-2.7.png)\n",
    "\n",
    "**Figure 2.7**: A univariate normal distribution has roughly $95\\%$ of its area in the range $|x - \\mu| \\leq 2\\sigma$, as shown. The peak of the distribution has value $p(\\mu) = \\frac{1}{\\sqrt{2\\pi}\\sigma}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4d09fb",
   "metadata": {},
   "source": [
    "## Multivariate Normal Density\n",
    "$$\n",
    "p(\\mathbf{x}) = \\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left[-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})\\right]\n",
    "$$\n",
    "- **Mean vector** ($\\boldsymbol{\\mu}$) and **covariance matrix** ($\\boldsymbol{\\Sigma}$) describe the distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## Formal Definitions\n",
    "\n",
    "Formally, we have:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu} = \\mathbb{E}[\\mathbf{x}] = \\int \\mathbf{x} \\, p(\\mathbf{x}) \\, d\\mathbf{x} \n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma} = \\mathbb{E}[(\\mathbf{x}-\\boldsymbol{\\mu})(\\mathbf{x}-\\boldsymbol{\\mu})^T]\n",
    "$$\n",
    "\n",
    "where the expected value of a vector or a matrix is found by taking the expected values of its components. In other words, if $x_i$ is the $i$th component of $\\mathbf{x}$, $\\mu_i$ the $i$th component of $\\boldsymbol{\\mu}$, and $\\sigma_{ij}$ the $ij$th component of $\\boldsymbol{\\Sigma}$, then:\n",
    "\n",
    "$$\n",
    "\\mu_i = \\mathbb{E}[x_i]\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\sigma_{ij} = \\mathbb{E}[(x_i - \\mu_i)(x_j - \\mu_j)]. \\quad \\text{(42)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Properties of the Covariance Matrix\n",
    "- The **covariance matrix** $\\boldsymbol{\\Sigma}$ is always **symmetric** and **positive semidefinite**.\n",
    "- We restrict our attention to the case where $\\boldsymbol{\\Sigma}$ is **positive definite**, so that the determinant of $\\boldsymbol{\\Sigma}$ is strictly positive.\n",
    "- The **diagonal elements** $\\sigma_{ii}$ are the **variances** of the respective $x_i$ (i.e., $\\sigma_i^2$).\n",
    "- The **off-diagonal elements** $\\sigma_{ij}$ are the **covariances** of $x_i$ and $x_j$.\n",
    "  - Example: For the length and weight features of a population of fish, we would expect a **positive covariance**.\n",
    "- If $x_i$ and $x_j$ are **statistically independent**, then $\\sigma_{ij} = 0$.\n",
    "- If **all off-diagonal elements are zero**, $p(\\mathbf{x})$ reduces to the product of the **univariate normal densities** for the components of $\\mathbf{x}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94efd89",
   "metadata": {},
   "source": [
    "## Multivariate Normal Density\n",
    "The **multivariate normal density** is completely specified by $d + \\frac{d(d+1)}{2}$ parameters:\n",
    "- The $d$ elements of the **mean vector** $\\boldsymbol{\\mu}$.\n",
    "- The $\\frac{d(d+1)}{2}$ independent elements of the **covariance matrix** $\\boldsymbol{\\Sigma}$.\n",
    "\n",
    "Samples drawn from a normal population tend to fall in a single **cluster** (see **Figure 2.9**):\n",
    "- The **center** of the cluster is determined by the mean vector $\\boldsymbol{\\mu}$.\n",
    "- The **shape** of the cluster is determined by the covariance matrix $\\boldsymbol{\\Sigma}$.\n",
    "\n",
    "The loci of points of constant density are **hyperellipsoids** defined by the quadratic form:\n",
    "$$\n",
    "(\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) = \\text{constant}.\n",
    "$$\n",
    "- The **principal axes** of these hyperellipsoids are given by the **eigenvectors** of $\\boldsymbol{\\Sigma}$ (denoted by $\\boldsymbol{\\Phi}$).\n",
    "- The **eigenvalues** (denoted by $\\boldsymbol{\\Lambda}$) determine the lengths of these axes.\n",
    "\n",
    "The quantity $r^2 = (\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})$\n",
    "is called the **squared Mahalanobis distance** from $\\mathbf{x}$ to $\\boldsymbol{\\mu}$. Thus:\n",
    "- The **contours of constant density** are hyperellipsoids of constant Mahalanobis distance to $\\boldsymbol{\\mu}$.\n",
    "- The **volume** of these hyperellipsoids measures the scatter of the samples about the mean.\n",
    "\n",
    "\n",
    "![Figure 2.9](img/Duda-Figure-2.9.png)\n",
    "\n",
    "**Figure 2.9**: Samples drawn from a two-dimensional Gaussian lie in a cloud centered on the mean $\\boldsymbol{\\mu}$. The red ellipses show lines of equal probability density of the Gaussian.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee908c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Discriminant Functions for Normal Density\n",
    "### Case 1: Equal Covariance Matrices ($\\boldsymbol{\\Sigma}_i = \\sigma^2 \\mathbf{I}$):\n",
    "  - The discriminant function is linear:\n",
    "\n",
    "    $$\n",
    "    g_i(\\mathbf{x}) = -\\frac{\\|\\mathbf{x}-\\boldsymbol{\\mu}_i\\|^2}{2\\sigma^2} + \\ln P(\\omega_i)\n",
    "    $$\n",
    "\n",
    "  - Decision boundaries are **hyperplanes**.\n",
    "  - If the **prior probabilities** are not equal, the squared distance $\\|\\mathbf{x}-\\boldsymbol{\\mu}_i\\|^2$ is normalized by the variance $\\sigma^2$ and offset by adding $\\ln P(\\omega_i)$. This means that if $\\mathbf{x}$ is equally near two different mean vectors, the optimal decision will favor the **a priori more likely category**.\n",
    "  - It is not necessary to compute distances explicitly. Expanding the quadratic form $(\\mathbf{x} - \\boldsymbol{\\mu}_i)^T (\\mathbf{x} - \\boldsymbol{\\mu}_i)$ yields:\n",
    "\n",
    "    $$\n",
    "    g_i(\\mathbf{x}) = -\\frac{1}{2\\sigma^2} \\left[ \\mathbf{x}^T \\mathbf{x} - 2 \\boldsymbol{\\mu}_i^T \\mathbf{x} + \\boldsymbol{\\mu}_i^T \\boldsymbol{\\mu}_i \\right] + \\ln P(\\omega_i)\n",
    "    $$\n",
    "\n",
    "    The quadratic term $\\mathbf{x}^T \\mathbf{x}$ is the same for all $i$, so it can be ignored as an additive constant. This simplifies the discriminant function to:\n",
    "    \n",
    "    $$\n",
    "    g_i(\\mathbf{x}) = \\mathbf{w}_i^T \\mathbf{x} + w_{i0},\n",
    "    $$\n",
    "    \n",
    "    where:\n",
    "    \n",
    "    $$\n",
    "    \\mathbf{w}_i = \\frac{\\boldsymbol{\\mu}_i}{\\sigma^2} \\quad \\text{(52)}\n",
    "    $$\n",
    "    \n",
    "    and\n",
    "    \n",
    "    $$\n",
    "    w_{i0} = -\\frac{\\boldsymbol{\\mu}_i^T \\boldsymbol{\\mu}_i}{2\\sigma^2} + \\ln P(\\omega_i) \n",
    "    $$\n",
    "    \n",
    "    Here, $w_{i0}$ is called the **threshold** or **bias** in the $i^{th}$ direction.\n",
    "  - A classifier that uses **linear discriminant functions** is called a **linear machine**. The decision surfaces for a linear machine are pieces of **hyperplanes** defined by the linear equations $g_i(\\mathbf{x}) = g_j(\\mathbf{x})$ for the two categories with the highest posterior probabilities. For our case, this equation can be written as:\n",
    "    \n",
    "    $$\n",
    "    \\mathbf{w}^T (\\mathbf{x} - \\mathbf{x}_0) = 0 \n",
    "    $$\n",
    "    \n",
    "    where:\n",
    "    \n",
    "    $$\n",
    "    \\mathbf{w} = \\boldsymbol{\\mu}_i - \\boldsymbol{\\mu}_j \n",
    "    $$\n",
    "    \n",
    "    and\n",
    "    \n",
    "    $$\n",
    "    \\mathbf{x}_0 = \\frac{1}{2} (\\boldsymbol{\\mu}_i + \\boldsymbol{\\mu}_j) - \\frac{\\sigma^2}{\\|\\boldsymbol{\\mu}_i - \\boldsymbol{\\mu}_j\\|^2} \\ln \\frac{P(\\omega_i)}{P(\\omega_j)}(\\boldsymbol{\\mu}_i - \\boldsymbol{\\mu}_j) \\quad \\text{(56)}.\n",
    "    $$\n",
    "    \n",
    "    This defines a **hyperplane** through the point $\\mathbf{x}_0$ and orthogonal to the vector $\\mathbf{w}$. Since $\\mathbf{w} = \\boldsymbol{\\mu}_i - \\boldsymbol{\\mu}_j$, the hyperplane separating $\\mathcal{R}_i$ and $\\mathcal{R}_j$ is orthogonal to the line linking the means. If $P(\\omega_i) = P(\\omega_j)$, the hyperplane is the **perpendicular bisector** of the line between the means. If the prior probabilities are unequal, the point $\\mathbf{x}_0$ shifts away from the more likely mean.\n",
    "\n",
    "---\n",
    "\n",
    "![Figure 2.10](img/Duda-Figure-2.10.png)\n",
    "\n",
    "**Figure 2.10**: If the covariances of two distributions are equal and proportional to the identity matrix, the distributions are spherical in $d$ dimensions, and the boundary is a generalized hyperplane of $d-1$ dimensions, perpendicular to the line separating the means.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c456d23",
   "metadata": {},
   "source": [
    "The equation $\\mathbf{w}^T (\\mathbf{x} - \\mathbf{x}_0) = 0$ represents a **hyperplane** in $n$-dimensional space. Here's how to interpret and visualize it:\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "1. $\\mathbf{w}$: A normal vector to the hyperplane (defines the orientation of the hyperplane).\n",
    "2. $\\mathbf{x}_0$: A fixed point on the hyperplane.\n",
    "3. $\\mathbf{x}$: A variable point on the hyperplane.\n",
    "\n",
    "The equation states that the vector $\\mathbf{x} - \\mathbf{x}_0$ is perpendicular to $\\mathbf{w}$, meaning all points $\\mathbf{x}$ on the hyperplane satisfy this condition.\n",
    "\n",
    "---\n",
    "\n",
    "**Visualization in 2D**\n",
    "In 2D space ($n = 2$), the equation reduces to a **line**. Let’s break it down:\n",
    "\n",
    "1. $\\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}$: The normal vector to the line.\n",
    "2. $\\mathbf{x}_0 = \\begin{bmatrix} x_0 \\\\ y_0 \\end{bmatrix}$: A fixed point on the line.\n",
    "3. $\\mathbf{x} = \\begin{bmatrix} x \\\\ y \\end{bmatrix}$: A variable point on the line.\n",
    "\n",
    "The equation becomes:\n",
    "$w_1 (x - x_0) + w_2 (y - y_0) = 0$\n",
    "\n",
    "This is the equation of a line in 2D.\n",
    "\n",
    "---\n",
    "\n",
    "**Drawing the Hyperplane (Line in 2D)**\n",
    "Here’s how to draw it:\n",
    "\n",
    "1. **Plot the point $\\mathbf{x}_0$:** This is a fixed point on the line.\n",
    "2. **Draw the normal vector $\\mathbf{w}$:** This vector is perpendicular to the line.\n",
    "3. **Draw the line:** The line passes through $\\mathbf{x}_0$ and is perpendicular to $\\mathbf{w}$.\n",
    "\n",
    "---\n",
    "\n",
    "**Example**\n",
    "Let’s use the following values:\n",
    "- $\\mathbf{w} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$ (normal vector).\n",
    "- $\\mathbf{x}_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ (a point on the line).\n",
    "\n",
    "The equation becomes:\n",
    "$2(x - 1) + 1(y - 1) = 0$\n",
    "Simplify:\n",
    "$2x - 2 + y - 1 = 0 \\implies 2x + y - 3 = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5abda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the normal vector w and point x0\n",
    "w = np.array([2, 1])  # Normal vector\n",
    "x0 = np.array([1, 1])  # Point on the line\n",
    "\n",
    "# Define the line equation: 2x + y - 3 = 0 => y = -2x + 3\n",
    "x_values = np.linspace(-5, 5, 100)  # Range of x values\n",
    "y_values = -2 * x_values + 3  # Corresponding y values\n",
    "\n",
    "# Plot the line\n",
    "plt.plot(x_values, y_values, label=\"Line: $2x + y - 3 = 0$\")\n",
    "\n",
    "# Plot the point x0\n",
    "plt.scatter(x0[0], x0[1], color=\"red\", label=\"Point $\\mathbf{x}_0 = (1, 1)$\")\n",
    "\n",
    "# Plot the normal vector w starting from x0\n",
    "plt.quiver(x0[0], x0[1], w[0], w[1], angles='xy', scale_units='xy', scale=1, color=\"green\", label=\"Normal vector $\\mathbf{w} = (2, 1)$\")\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.axhline(0, color=\"black\", linewidth=0.5)  # x-axis\n",
    "plt.axvline(0, color=\"black\", linewidth=0.5)  # y-axis\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title(\"Line and Normal Vector Visualization\")\n",
    "plt.xlim(-5, 5)  # Adjust x-axis limits\n",
    "plt.ylim(-5, 5)  # Adjust y-axis limits\n",
    "plt.gca().set_aspect(\"equal\", adjustable=\"box\")  # Equal aspect ratio\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb32073",
   "metadata": {},
   "source": [
    "**Hyperplane and Decision Boundary**\n",
    "\n",
    "The equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^T (\\mathbf{x} - \\mathbf{x}_0) = 0\n",
    "$$\n",
    "\n",
    "defines a **hyperplane** through the point $\\mathbf{x}_0$ and orthogonal to the vector $\\mathbf{w}$. Since $\\mathbf{w} = \\boldsymbol{\\mu}_i - \\boldsymbol{\\mu}_j$, the hyperplane separating $\\mathcal{R}_i$ and $\\mathcal{R}_j$ is orthogonal to the line linking the means. \n",
    "\n",
    "- If $P(\\omega_i) = P(\\omega_j)$, the second term in **Eq. 56** vanishes, and the point $\\mathbf{x}_0$ is **halfway between the means**. In this case, the hyperplane is the **perpendicular bisector** of the line between the means (see **Figure 2.11**).\n",
    "- If the prior probabilities are **unequal**, the point $\\mathbf{x}_0$ shifts away from the more likely mean.\n",
    "- Note that if the variance $\\sigma^2$ is small relative to the squared distance $\\|\\boldsymbol{\\mu}_i - \\boldsymbol{\\mu}_j\\|^2$, the position of the decision boundary is relatively **insensitive** to the exact values of the prior probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "**Minimum Distance Classifier**\n",
    "- If the **prior probabilities** $P(\\omega_i)$ are the same for all $c$ classes, the term $\\ln P(\\omega_i)$ becomes an unimportant additive constant and can be ignored.\n",
    "- In this case, the **optimum decision rule** simplifies to:\n",
    "  - Measure the **Euclidean distance** $\\|\\mathbf{x} - \\boldsymbol{\\mu}_i\\|$ from $\\mathbf{x}$ to each of the $c$ mean vectors.\n",
    "  - Assign $\\mathbf{x}$ to the category of the **nearest mean**.\n",
    "- Such a classifier is called a **minimum distance classifier**.\n",
    "- If each mean vector is considered an **ideal prototype** or **template** for patterns in its class, this is essentially a **template-matching procedure** (see **Figure 2.10**). This technique is similar to **nearest-neighbor algorithm** visited in the previous chapter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57163f7",
   "metadata": {},
   "source": [
    "![Figure 2.11](img/Duda-Figure-2.11.png)\n",
    "\n",
    "Figure 2.11: As the priors are changed, the decision boundary shifts; for sufficiently disparate priors the boundary will not lie between the means of these 1-, 2- and 3-dimensional spherical Gaussian distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d375c13f",
   "metadata": {},
   "source": [
    "**Exercise: Validating the Decision Boundary in Case 2 and Comparing with GaussianNB**\n",
    "\n",
    "**Objective**\n",
    "In this exercise, you will:\n",
    "1. Generate two groups of data with Gaussian distributions.\n",
    "2. Compute the decision boundary for **Case 2** (equal but arbitrary covariance matrices) using the theoretical approach.\n",
    "3. Compare the results with the decision boundary generated by Scikit-Learn's `GaussianNB`.\n",
    "4. Visualize the results in a single plot or side-by-side plots.\n",
    "\n",
    "---\n",
    "\n",
    "**Steps to Follow**\n",
    "\n",
    "1. **Generate Data**:\n",
    "   - Create two classes of data points, each following a **multivariate Gaussian distribution** with the same covariance matrix but different mean vectors.\n",
    "   - Use the following parameters for the two classes:\n",
    "     - Class 1: Mean vector $\\boldsymbol{\\mu}_1 = [2, 3]$, Covariance matrix $\\boldsymbol{\\Sigma} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}$.\n",
    "     - Class 2: Mean vector $\\boldsymbol{\\mu}_2 = [6, 5]$, Covariance matrix $\\boldsymbol{\\Sigma} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}$.\n",
    "   - Generate 200 data points for each class.\n",
    "\n",
    "2. **Compute the Decision Boundary (Theoretical Approach)**:\n",
    "   - Use the **linear discriminant function** for Case 2:\n",
    "     $$\n",
    "     g_i(\\mathbf{x}) = -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_i)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_i) + \\ln P(\\omega_i).\n",
    "     $$\n",
    "   - Assume equal prior probabilities ($P(\\omega_1) = P(\\omega_2) = 0.5$).\n",
    "   - The decision boundary is the set of points where $g_1(\\mathbf{x}) = g_2(\\mathbf{x})$.\n",
    "\n",
    "3. **Compute the Decision Boundary (GaussianNB)**:\n",
    "   - Use Scikit-Learn's `GaussianNB` to fit the data and predict the labels for test points.\n",
    "\n",
    "4. **Generate Test Points**:\n",
    "   - Create a grid of test points covering the entire plot area (e.g., using `np.meshgrid`).\n",
    "   - Predict the class labels for these test points using both the theoretical decision boundary and `GaussianNB`.\n",
    "\n",
    "5. **Visualize the Results**:\n",
    "   - Plot the original data points for both classes.\n",
    "   - Plot the test points with their predicted labels (use transparency to avoid obscuring the original data).\n",
    "   - Draw the **decision boundary** for both the theoretical approach and `GaussianNB`.\n",
    "   - Draw the **line connecting the two class centers** and the **perpendicular bisector**.\n",
    "\n",
    "---\n",
    "\n",
    "**Expected Output**\n",
    "- A figure with two subplots:\n",
    "  1. **Theoretical Decision Boundary**:\n",
    "     - The original data points for both classes.\n",
    "     - The test points with their predicted labels (shaded regions).\n",
    "     - The theoretical decision boundary (black line).\n",
    "     - The line connecting the two class centers (dashed black line).\n",
    "     - The perpendicular bisector (dotted green line).\n",
    "  2. **GaussianNB Decision Boundary**:\n",
    "     - The original data points for both classes.\n",
    "     - The test points with their predicted labels (shaded regions).\n",
    "     - The decision boundary generated by `GaussianNB` (black line).\n",
    "     - The line connecting the two class centers (dashed black line).\n",
    "     - The perpendicular bisector (dotted green line).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dcf106",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Case 2: Equal but Arbitrary Covariance Matrices ($ \\boldsymbol{\\Sigma}_i = \\boldsymbol{\\Sigma} $):\n",
    "  - Discriminant function is still linear:\n",
    "    $$\n",
    "    g_i(\\mathbf{x}) = -\\frac{1}{2} (\\mathbf{x}-\\boldsymbol{\\mu}_i)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_i) + \\ln P(\\omega_i)\n",
    "    $$\n",
    "  - Decision boundaries are hyperplanes, but not necessarily orthogonal to the line between means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c56779",
   "metadata": {},
   "source": [
    "\n",
    "### Case 3: Arbitrary Covariance Matrices ($ \\boldsymbol{\\Sigma}_i $):\n",
    "  - Discriminant function is quadratic:\n",
    "\n",
    "    $$\n",
    "    g_i(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{W}_i \\mathbf{x} + \\mathbf{w}_i^T \\mathbf{x} + w_{i0}\n",
    "    $$\n",
    "  \n",
    "    where:\n",
    "  \n",
    "    $$\n",
    "    \\mathbf{W}_i = -\\frac{1}{2} \\boldsymbol{\\Sigma}_i^{-1}, \\quad \\mathbf{w}_i = \\boldsymbol{\\Sigma}_i^{-1} \\boldsymbol{\\mu}_i, \\quad w_{i0} = -\\frac{1}{2} \\boldsymbol{\\mu}_i^T \\boldsymbol{\\Sigma}_i^{-1} \\boldsymbol{\\mu}_i - \\frac{1}{2} \\ln |\\boldsymbol{\\Sigma}_i| + \\ln P(\\omega_i)\n",
    "    $$\n",
    "  <!-- - Decision boundaries are hyperquadrics (e.g., hyperplanes, hyperellipsoids, hyperparaboloids).\n",
    "  - ![Figure 2.14](img/Duda-Figure-2.14.png)\n",
    "  - ![Figure 2.15](img/Duda-Figure-2.15.png) -->\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "- Bayesian Decision Theory provides a framework for optimal classification under uncertainty.\n",
    "- Key components:\n",
    "  - Prior probabilities.\n",
    "  - Likelihoods (class-conditional densities).\n",
    "  - Posterior probabilities.\n",
    "- Decision rules minimize error rates or expected loss.\n",
    "- Normal distributions are commonly used due to their mathematical tractability.\n",
    "\n",
    "---\n",
    "<!-- \n",
    "## **12. Example: Two-Category Classification**\n",
    "- Given two classes $ \\omega_1 $ and $ \\omega_2 $ with:\n",
    "  - Mean vectors:\n",
    "    $$\n",
    "    \\boldsymbol{\\mu}_1 = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}, \\quad \\boldsymbol{\\mu}_2 = \\begin{bmatrix} 3 \\\\ -2 \\end{bmatrix}\n",
    "    $$\n",
    "  - Covariance matrices:\n",
    "    $$\n",
    "    \\boldsymbol{\\Sigma}_1 = \\begin{bmatrix} 1/2 & 0 \\\\ 0 & 2 \\end{bmatrix}, \\quad \\boldsymbol{\\Sigma}_2 = \\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix}\n",
    "    $$\n",
    "  - Equal prior probabilities: $ P(\\omega_1) = P(\\omega_2) = 0.5 $.\n",
    "- The decision boundary is a parabola:\n",
    "  $$\n",
    "  x_2 = 3.514 - 1.125 x_1 + 0.1875 x_1^2\n",
    "  $$\n",
    "  - ![Figure 2.16](img/Duda-Figure-2.16.png)\n",
    "\n",
    "--- -->\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
