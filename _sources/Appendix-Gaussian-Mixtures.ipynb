{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3494d7ba",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087c0aa5",
   "metadata": {},
   "source": [
    "The contents of this section are derived from Chapter 13 of *Data Mining and Machine Learning* by Mohammed J. Zaki and Wagner Meira Jr. (2020) {cite}`zaki2020data`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3136f8",
   "metadata": {},
   "source": [
    "\n",
    "## K-means Algorithm: Review\n",
    "\n",
    "The *sum of squared errors* scoring function is defined as  \n",
    "\n",
    "$$\n",
    "SSE(\\mathcal{C}) = \\sum_{i=1}^{k}\\sum_{\\mathbf{x}_{j} \\in\n",
    "C_{i}}\\|\\mathbf{x}_{j}-\\mathbf{\\mu}_i\\|^2\n",
    "$$\n",
    "\n",
    "The goal is to find the clustering that minimizes the SSE score:\n",
    "\n",
    "$$\n",
    "\\mathcal{C}^* = \\arg \\min_{\\mathcal{C}} \\{ SSE(\\mathcal{C}) \\}\n",
    "$$\n",
    "\n",
    "K-means employs a greedy iterative approach to find a clustering that minimizes the SSE objective. As such, it can converge to a local optimum instead of a globally optimal clustering.\n",
    "\n",
    "---\n",
    "\n",
    "## K-means Algorithm: Steps\n",
    "\n",
    "K-means initializes the cluster means by randomly generating $k$ points in the data space. Each iteration consists of two steps:  \n",
    "\n",
    "1. **Cluster Assignment:** Each point $\\mathbf{x}_{j} \\in \\mathbf{D}$ is assigned to the closest mean, inducing a clustering with each cluster $C_i$ comprising points closest to $\\mathbf{\\mu}_i$:\n",
    "\n",
    "   $$ \n",
    "   i^* = \\arg \\min_{i=1}^k \\Bigl\\{\\|\\mathbf{x}_{j} - \\mathbf{\\mu}_i\\|^2\\Bigr\\}\n",
    "   $$\n",
    "\n",
    "2. **Centroid Update:** New mean values are computed for each cluster from the points in $C_i$.  \n",
    "\n",
    "These steps iterate until convergence to a fixed point or local minimum.\n",
    "\n",
    "---\n",
    "\n",
    "## K-Means Algorithm \n",
    "![](img/Zaki-Chap13-KMeansAlgorithm.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0ff35a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Expectation-Maximization Clustering  \n",
    "\n",
    "Let $X_a$ denote the random variable corresponding to the $a^{th}$ attribute. Let $\\mathbf{X} = (X_1, X_2, \\dots, X_d)$ denote the vector random variable across the $d$ attributes, with $\\mathbf{x}_{j}$ being a data sample from $\\mathbf{X}$.  \n",
    "\n",
    "We assume each cluster $C_i$ is characterized by a multivariate normal distribution:\n",
    "\n",
    "$$\n",
    "f_i(\\mathbf{x}) = f(\\mathbf{x}|\\mathbf{\\mu}_i,\\mathbf{\\Sigma}_i)\n",
    "= \\frac{1}{(2\\pi)^\\frac{d}{2}|\\mathbf{\\Sigma}_i|^\\frac{1}{2}}\n",
    "    \\exp\\left\\{-\\frac{(\\mathbf{x}-\\mathbf{\\mu}_{i})^{T} \\mathbf{\\Sigma}_i^{-1}\n",
    "    (\\mathbf{x}-\\mathbf{\\mu}_{i})}{2}\\right\\}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\mu}_i \\in \\mathbb{R}^d$ is the cluster mean and $\\mathbf{\\Sigma}_i \\in \\mathbb{R}^{d \\times d}$ is the covariance matrix.\n",
    "\n",
    "\n",
    "The probability density function of $\\mathbf{X}$  is given as a *Gaussian mixture model* over all the $k$ clusters:  \n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\sum_{i=1}^k f_i(\\mathbf{x}) P(C_i)\n",
    "= \\sum_{i=1}^k f(\\mathbf{x} \\mid \\mathbf{\\mu}_i, \\mathbf{\\Sigma}_i) P(C_i)\n",
    "$$\n",
    "\n",
    "where the prior probabilities $P(C_i)$ are called the  *mixture parameters*, which must satisfy the condition:  \n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^k P(C_i) = 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f01755",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation  \n",
    "\n",
    "We write the set of all model parameters compactly as:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\theta} = \\left\\{\\mathbf{\\mu}_1,\\mathbf{\\Sigma}_1,P(C_1), \\dots, \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k, P(C_k)\\right\\}\n",
    "$$\n",
    "\n",
    "Given the dataset $\\mathbf{D}$, the *likelihood* of $\\mathbf{\\theta}$ is:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{D} | \\mathbf{\\theta}) = \\prod_{j=1}^n f(\\mathbf{x}_{j})\n",
    "$$\n",
    "\n",
    "MLE chooses parameters $\\mathbf{\\theta}$ to maximize likelihood:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\theta}^* = \\arg\\max_{\\mathbf{\\theta}} \\{\\ln P(\\mathbf{D} | \\mathbf{\\theta})\\}\n",
    "$$\n",
    "\n",
    "where the log-likelihood function is:\n",
    "\n",
    "$$\n",
    "\\ln P(\\mathbf{D} | \\mathbf{\\theta}) = \\sum_{j=1}^n \\ln f(\\mathbf{x}_{j})\n",
    "    = \\sum_{j=1}^n \\ln \\biggl( \\sum_{i=1}^k f(\\mathbf{x}_{j} | \\mathbf{\\mu}_i, \\mathbf{\\Sigma}_i)P(C_i)\\biggr)\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d0ea7b",
   "metadata": {},
   "source": [
    "### **Maximizing Log-Likelihood with Expectation-Maximization (EM)**  \n",
    "\n",
    "Directly maximizing the log-likelihood over $\\mathbf{\\theta}$ is difficult because of the **summation inside the logarithm**:  \n",
    "\n",
    "$$\n",
    "\\ln P(\\mathbf{D} | \\mathbf{\\theta}) = \\sum_{j=1}^n \\ln \\left( \\sum_{i=1}^k f(\\mathbf{x}_{j} | \\mathbf{\\mu}_i, \\mathbf{\\Sigma}_i) P(C_i) \\right)\n",
    "$$\n",
    "\n",
    "If the likelihood function involved only a **single Gaussian**, direct differentiation would be straightforward. However, in a Gaussian mixture model, the presence of multiple Gaussians inside the summation complicates differentiation. Since each data point $\\mathbf{x}_j$ does not belong to one cluster deterministically but instead has a probability of belonging to multiple clusters, it is difficult to solve for the parameters analytically.\n",
    "\n",
    "#### **How does EM help?**  \n",
    "\n",
    "Expectation-Maximization provides a structured approach to **indirectly** maximize the likelihood by introducing latent variablesâ€”the cluster assignments $w_{ij} = P(C_i | \\mathbf{x}_j)$. The **Expectation step** computes these probabilities, effectively estimating the hidden memberships, while the **Maximization step** updates the parameters using these weights.\n",
    "\n",
    "By iterating between these steps, EM avoids the complexity of differentiating the nested sum-log expression directly, making parameter estimation feasible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40984b6a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Expectation-Maximization for Gaussian Mixture Models  \n",
    "\n",
    "### **1D Case: Univariate Normal Clustering**  \n",
    "\n",
    "Let $\\mathbf{D}$ comprise a single attribute $X$, where each point $x_{j} \\in \\mathbb{R}$ is a random sample from $X$. The mixture model uses univariate normal distributions for each cluster:  \n",
    "\n",
    "$$\n",
    "f_i(x) = f(x | \\mu_i, \\sigma_i^2) =\n",
    "  \\frac{1}{\\sqrt{2\\pi}\\sigma_i} \\exp \\left\\{-\\frac{(x -\n",
    "  \\mu_i)^2}{2\\sigma_i^2} \\right\\}\n",
    "$$\n",
    "\n",
    "where the cluster parameters include $\\mu_{i}$ (mean), $\\sigma_{i}^2$ (variance), and $P(C_i)$ (prior probability).  \n",
    "\n",
    "#### **Initialization**  \n",
    "Each cluster $C_i$ (for $i = 1, 2, \\dots, k$) is randomly initialized with parameters $\\mu_{i}$, $\\sigma_{i}^2$, and $P(C_i)$.  \n",
    "\n",
    "#### **Expectation Step**  \n",
    "Given the mean $\\mu_i$, variance $\\sigma_i^2$, and prior probability $P(C_i)$ for each cluster, the **cluster posterior probability** is computed using Bayes' theorem:  \n",
    "\n",
    "$$\n",
    "w_{ij} = P(C_{i}|x_{j}) =\n",
    "    \\frac{f(x_{j}|\\mu_i,\\sigma_i^2) \\cdot P(C_{i})}\n",
    "    {\\sum_{a=1}^k f(x_{j} |\\mu_a, \\sigma_a^2) \\cdot P(C_{a})}\n",
    "$$\n",
    "\n",
    "#### **Maximization Step**  \n",
    "Using $ w_{ij} $, the updated cluster parameters are computed as follows:  \n",
    "\n",
    "- **Mean update** (weighted average of points):  \n",
    "\n",
    "  $$\n",
    "  \\mu_{i} = \\frac{\\sum_{j=1}^n w_{ij} \\cdot x_{j}}\n",
    "    {\\sum_{j=1}^n w_{ij}}\n",
    "  $$\n",
    "\n",
    "- **Variance update** (weighted variance of points):  \n",
    "\n",
    "  $$\n",
    "  \\sigma_{i}^{2} =\n",
    "    \\frac{\\sum_{j=1}^n w_{ij}(x_{j}-\\mu_{i})^2}\n",
    "        {\\sum_{j=1}^n w_{ij}}\n",
    "  $$\n",
    "\n",
    "- **Prior probability update** (fraction of weights contributing to the cluster):  \n",
    "\n",
    "  $$\n",
    "  P(C_{i}) =\n",
    "    \\frac{\\sum_{j=1}^n w_{ij}}{n}\n",
    "  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b875a2",
   "metadata": {},
   "source": [
    "\n",
    "![](img/Zaki-Chap13-EM1D.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f682ede",
   "metadata": {},
   "source": [
    "\n",
    "### **Multivariate Case: Full Covariance Gaussian Mixture Model**  \n",
    "\n",
    "In the **multivariate case**, each cluster $C_i$ is modeled by a multivariate normal distribution, extending beyond the 1D case:  \n",
    "\n",
    "$$\n",
    "f_i(\\mathbf{x}) = f(\\mathbf{x}|\\mathbf{\\mu}_i,\\mathbf{\\Sigma}_i)\n",
    "= \\frac{1}{(2\\pi)^\\frac{d}{2}|\\mathbf{\\Sigma}_i|^\\frac{1}{2}}\n",
    "    \\exp\\left\\{-\\frac{(\\mathbf{x}-\\mathbf{\\mu}_{i})^{T} \\mathbf{\\Sigma}_i^{-1}\n",
    "    (\\mathbf{x}-\\mathbf{\\mu}_{i})}{2}\\right\\}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\mu}_i \\in \\mathbb{R}^d$ is the cluster mean and $\\mathbf{\\Sigma}_i \\in \\mathbb{R}^{d \\times d}$ is the covariance matrix.  \n",
    "\n",
    "#### **Expectation Step (Multivariate)**  \n",
    "Similar to the univariate case, the cluster posterior probability is computed using Bayes' theorem:  \n",
    "\n",
    "$$\n",
    "w_{ij} = P(C_i | \\mathbf{x}_{j}) = \\frac{f_i(\\mathbf{x}_{j}) \\cdot P(C_{i})}\n",
    "    {\\sum_{a=1}^k f_a(\\mathbf{x}_{j}) \\cdot P(C_{a})}\n",
    "$$\n",
    "\n",
    "#### **Maximization Step (Multivariate)**  \n",
    "\n",
    "- **Mean update** (weighted average of points):  \n",
    "\n",
    "  $$\n",
    "  \\mathbf{\\mu}_{i} = \\frac{\\sum_{j=1}^n P(C_i | \\mathbf{x}_j) \\cdot \\mathbf{x}_{j}}\n",
    "  {\\sum_{j=1}^n P(C_i | \\mathbf{x}_j)}\n",
    "  $$\n",
    "\n",
    "- **Covariance update** (weighted covariance matrix across dimensions):  \n",
    "\n",
    "  $$\n",
    "  \\mathbf{\\Sigma}_{i} =\n",
    "    \\frac{\\sum_{j=1}^n P(C_i | \\mathbf{x}_j) (\\mathbf{x}_j - \\mathbf{\\mu}_i)(\\mathbf{x}_j - \\mathbf{\\mu}_i)^T}\n",
    "    {\\sum_{j=1}^n P(C_i | \\mathbf{x}_j)}\n",
    "  $$\n",
    "\n",
    "- **Prior probability update** (fraction of total contribution to the cluster):  \n",
    "\n",
    "  $$\n",
    "  P(C_{i}) =\n",
    "    \\frac{\\sum_{j=1}^n P(C_i | \\mathbf{x}_j)}{n}\n",
    "  $$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fcb937",
   "metadata": {},
   "source": [
    "## Expectation-Maximization Clustering Algorithm\n",
    "\n",
    "![](img/Zaki-Chap13-EMAlgorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35014d3e",
   "metadata": {},
   "source": [
    "### **K-Means as a Special Case of Expectation-Maximization (EM)**  \n",
    "\n",
    "K-means can be viewed as a **simplified version** of the Expectation-Maximization (EM) algorithm, where cluster assignments are deterministic rather than probabilistic. Specifically, in K-means, each data point $\\mathbf{x}_{j}$ belongs exclusively to one cluster based on the minimum squared distance to the cluster centroid:  \n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}_{j} | C_i) =\n",
    "\\begin{cases}\n",
    "  1 & \\text{if } C_i = \\arg\\min_{C_a} \\left\\{\n",
    "    \\|\\mathbf{x}_{j} - \\mathbf{\\mu}_a\\|^2\\right\\}\\\\\n",
    "  0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The posterior probability of cluster assignment is calculated using Bayes' theorem:  \n",
    "\n",
    "$$\n",
    "P(C_i | \\mathbf{x}_{j}) = \\frac{P(\\mathbf{x}_{j} | C_i) P(C_i)}\n",
    "{\\sum_{a=1}^k P(\\mathbf{x}_{j} | C_a) P(C_a)}\n",
    "$$\n",
    "\n",
    "Since K-means deterministically assigns each point to exactly one cluster, this simplifies to the following cases:\n",
    "\n",
    "- If $P(\\mathbf{x}_{j} | C_i) = 0$, then $P(C_i | \\mathbf{x}_{j}) = 0$.\n",
    "- If $P(\\mathbf{x}_{j} | C_i) = 1$, then for all $a \\neq i$, we have $P(\\mathbf{x}_{j} | C_a) = 0$, so:\n",
    "\n",
    "  $$\n",
    "  P(C_i | \\mathbf{x}_{j}) = \\frac{1 \\cdot P(C_i)}{1 \\cdot P(C_i)} = 1.\n",
    "  $$\n",
    "\n",
    "Thus, in K-means, cluster membership is binary:\n",
    "\n",
    "$$\n",
    "P(C_i | \\mathbf{x}_{j}) =\n",
    "\\begin{cases}\n",
    "  1 & \\text{if } \\mathbf{x}_{j} \\in C_i, \\text{i.e., if }\n",
    "  C_i = \\arg\\min_{C_a} \\left\\{ \\|\\mathbf{x}_{j} - \\mathbf{\\mu}_a\\|^2\\right\\}\\\\\n",
    "  0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Unlike EM, which incorporates covariance matrices to model cluster distributions, K-means **only estimates the cluster centroids $\\mathbf{\\mu}_i$ and the cluster proportions $P(C_i)$**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
