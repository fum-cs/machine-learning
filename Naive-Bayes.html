
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Naive Bayes Classification &#8212; ML</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Naive-Bayes';</script>
    <link rel="icon" href="_static/fum-logo.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Bayesian Decision Theory" href="Bayesian-Decision-Theory.html" />
    <link rel="prev" title="Introduction" href="01%20-%20Introduction.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/fum-cs-logo.png" class="logo__image only-light" alt="ML - Home"/>
    <script>document.write(`<img src="_static/fum-cs-logo.png" class="logo__image only-dark" alt="ML - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01%20-%20Introduction.html">Introduction</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Naive Bayes Classification</a></li>


<li class="toctree-l1"><a class="reference internal" href="Bayesian-Decision-Theory.html">Bayesian Decision Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="MLE-introduction.html">Maximum Likelihood Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Mahalanobis-Distance.html">Mahalanobis Distance</a></li>
<li class="toctree-l1"><a class="reference internal" href="Feature-Map.html">Feature Maps: Bridging to Kernel Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="Kernel-Trick.html">The Kernel Method (Kernel Trick)</a></li>
<li class="toctree-l1"><a class="reference internal" href="02%20-%20Linear%20Models.html">Linear models</a></li>

<li class="toctree-l1"><a class="reference internal" href="04%20-%20Model%20Selection.html">Model Selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="05%20-%20Ensemble%20Learning.html">Ensemble Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="06%20-%20Data%20Preprocessing.html">Data preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="07%20-%20Bayesian%20Learning.html">Gaussian processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="08%20-%20Hidden%20Markov%20Models.html">Hidden Markov Models</a></li>


<li class="toctree-l1"><a class="reference internal" href="AppenixA-Kernel-SVM-and-Kernel-Regression.html">Appendix A: Kernel SVM and Kernel Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="Appendix-BDT-Discrete-Features.html">Appendix: Bayes Decision Theory — Discrete Features (Based on Duda et al., Section 2.9)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/fum-cs/machine-learning/blob/main/notebooks/Naive-Bayes.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/fum-cs/machine-learning" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/fum-cs/machine-learning/issues/new?title=Issue%20on%20page%20%2FNaive-Bayes.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Naive-Bayes.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Naive Bayes Classification</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Naive Bayes Classification</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes-classifier">Naive Bayes Classifier</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#grasshoppers-vs-katydids">Grasshoppers vs. Katydids</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#antenna-length-vs-abdomen-length">Antenna Length vs. Abdomen Length</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classifying-an-insect">Classifying an Insect</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-antennae-length-is-3">Example: Antennae Length is 3</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-classifiers">Bayes Classifiers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-classifiers">Bayesian Classifiers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-classifiers-and-bayes-theorem">Bayesian Classifiers and Bayes’ Theorem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simplified-formula-for-classification">Simplified Formula for Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-classifying-by-name">Example: Classifying by Name</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#officer-drew-example">Officer Drew Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-probabilities-for-officer-drew">Calculating Probabilities for Officer Drew</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes-with-multiple-features">Naive Bayes with Multiple Features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-officer-drew-with-multiple-features">Example: Officer Drew with Multiple Features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-probabilities-for-officer-drew-with-multiple-features">Calculating Probabilities for Officer Drew with Multiple Features</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes-classification-with-multiple-features-fruit-example">Naive Bayes Classification with multiple features: Fruit Example</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-data">Training Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-calculate-prior-probabilities">Step 1: Calculate Prior Probabilities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-calculate-evidence-probabilities">Step 2: Calculate Evidence Probabilities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-calculate-likelihoods">Step 3: Calculate Likelihoods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#for-banana">For <strong>Banana</strong>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#for-orange">For <strong>Orange</strong>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#for-other-fruit">For <strong>Other Fruit</strong>:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-classify-a-new-fruit">Step 4: Classify a New Fruit</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculate-posterior-probabilities">Calculate Posterior Probabilities</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">For <strong>Banana</strong>:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">For <strong>Orange</strong>:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">For <strong>Other Fruit</strong>:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-make-a-prediction">Step 5: Make a Prediction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-takeaways">Key Takeaways</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-naive-bayes">Gaussian Naive Bayes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-naive-bayes">Multinomial Naive Bayes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-classifying-text">Example: Classifying Text</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-readning">Further Readning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-naive-bayes">When to Use Naive Bayes</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="naive-bayes-classification">
<h1>Naive Bayes Classification<a class="headerlink" href="#naive-bayes-classification" title="Link to this heading">#</a></h1>
<p>The previous four chapters have given a general overview of the concepts of machine learning.
In this chapter and the ones that follow, we will be taking a
closer look first at four algorithms for  supervised learning,
and then at four algorithms for unsupervised learning.
We start here with our first supervised method, naive Bayes classification.</p>
<p>Naive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets.
Because they are so fast and have so few tunable parameters, they end up being useful as a quick-and-dirty baseline for a classification problem.
This chapter will provide an intuitive explanation of how naive Bayes classifiers work, followed by a few examples of them in action on some datasets.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="naive-bayes-classifier">
<h1>Naive Bayes Classifier<a class="headerlink" href="#naive-bayes-classifier" title="Link to this heading">#</a></h1>
<p>by: <a class="reference external" href="https://www.cs.ucr.edu/~eamonn/">Eamonn Keogh</a>,
<a class="reference external" href="https://www1.cs.ucr.edu/">CS, UCR</a>. <a class="reference external" href="https://www.cs.ucr.edu/~eamonn/CE/Bayesian%20Classification%20withInsect_examples.pdf">PDF</a></p>
<p><img alt="" src="_images/BCW_page_01.jpg" /></p>
<p><strong>Thomas Bayes 1702 - 1761</strong></p>
<p>We will start off with a visual intuition before looking at the math…</p>
<hr class="docutils" />
<section id="grasshoppers-vs-katydids">
<h2>Grasshoppers vs. Katydids<a class="headerlink" href="#grasshoppers-vs-katydids" title="Link to this heading">#</a></h2>
<p><img alt="" src="_images/BCW_page_02.png" /></p>
<section id="antenna-length-vs-abdomen-length">
<h3>Antenna Length vs. Abdomen Length<a class="headerlink" href="#antenna-length-vs-abdomen-length" title="Link to this heading">#</a></h3>
<p>With a lot of data, we can build a histogram. Let us just build one for “Antenna Length” for now…</p>
<p><img alt="" src="_images/BCW_page_03.png" /></p>
<p><img alt="" src="_images/BCW_page_04.png" /></p>
</section>
<hr class="docutils" />
<section id="classifying-an-insect">
<h3>Classifying an Insect<a class="headerlink" href="#classifying-an-insect" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>We want to classify an insect we have found. Its antennae are 3 units long. How can we classify it?</p></li>
</ul>
<p><img alt="" src="_images/BCW_page_05.png" /></p>
<ul class="simple">
<li><p>We can just ask ourselves, given the distributions of antennae lengths we have seen, is it more probable that our insect is a <strong>Grasshopper</strong> or a <strong>Katydid</strong>.</p></li>
<li><p>There is a formal way to discuss the most probable classification:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p(c_j | d) = \text{probability of class } c_j, \text{ given that we have observed } d
\]</div>
</section>
<hr class="docutils" />
<section id="example-antennae-length-is-3">
<h3>Example: Antennae Length is 3<a class="headerlink" href="#example-antennae-length-is-3" title="Link to this heading">#</a></h3>
<p><img alt="" src="_images/BCW_page_06.png" /></p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}
P(\text{Grasshopper} | 3) = \frac{10}{10 + 2} = 0.833\\\end{split}\\P(\text{Katydid} | 3) = \frac{2}{10 + 2} = 0.166
\end{aligned}\end{align} \]</div>
<!-- ---

### Example: Antennae Length is 7

$$
P(\text{Grasshopper} | 7) = \frac{3}{3 + 9} = 0.250
$$

$$
P(\text{Katydid} | 7) = \frac{9}{3 + 9} = 0.750
$$

---

### Example: Antennae Length is 5

$$
P(\text{Grasshopper} | 5) = \frac{6}{6 + 6} = 0.500
$$

$$
P(\text{Katydid} | 5) = \frac{6}{6 + 6} = 0.500
$$ -->
<p><img alt="" src="_images/BCW_page_07.png" /></p>
<p><img alt="" src="_images/BCW_page_08.png" /></p>
</section>
</section>
<hr class="docutils" />
<section id="bayes-classifiers">
<h2>Bayes Classifiers<a class="headerlink" href="#bayes-classifiers" title="Link to this heading">#</a></h2>
<p>That was a visual intuition for a simple case of the Bayes classifier, also called:</p>
<ul class="simple">
<li><p>Idiot Bayes</p></li>
<li><p>Naïve Bayes</p></li>
<li><p>Simple Bayes</p></li>
</ul>
<p>We are about to see some of the mathematical formalisms, and more examples, but keep in mind the basic idea.</p>
<p><em>Find out the probability of the previously unseen instance belonging to each class, then simply pick the most probable class.</em></p>
<hr class="docutils" />
<section id="bayesian-classifiers">
<h3>Bayesian Classifiers<a class="headerlink" href="#bayesian-classifiers" title="Link to this heading">#</a></h3>
<p>Here’s the modified version of your text, explicitly specifying <strong>prior probability</strong>, <strong>likelihood</strong>, and <strong>evidence</strong> in the context of Bayes’ Theorem:</p>
</section>
<hr class="docutils" />
<section id="bayesian-classifiers-and-bayes-theorem">
<h3>Bayesian Classifiers and Bayes’ Theorem<a class="headerlink" href="#bayesian-classifiers-and-bayes-theorem" title="Link to this heading">#</a></h3>
<p>Bayesian classifiers use <strong>Bayes’ Theorem</strong>, which is stated as:</p>
<div class="math notranslate nohighlight">
\[
p(c_j | d) = \frac{p(d | c_j) \, p(c_j)}{p(d)}
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><strong>Posterior Probability</strong> (<span class="math notranslate nohighlight">\( p(c_j | d) \)</span>):<br />
The probability of class <span class="math notranslate nohighlight">\( c_j \)</span> given the observed instance <span class="math notranslate nohighlight">\( d \)</span>. This is what we are trying to compute.</p></li>
<li><p><strong>Likelihood</strong> (<span class="math notranslate nohighlight">\( p(d | c_j) \)</span>):<br />
The probability of observing instance <span class="math notranslate nohighlight">\( d \)</span> given that it belongs to class <span class="math notranslate nohighlight">\( c_j \)</span>. This represents how likely the evidence (features of <span class="math notranslate nohighlight">\( d \)</span>) is under class <span class="math notranslate nohighlight">\( c_j \)</span>.</p></li>
<li><p><strong>Prior Probability</strong> (<span class="math notranslate nohighlight">\( p(c_j) \)</span>):<br />
The probability of class <span class="math notranslate nohighlight">\( c_j \)</span> occurring in the dataset. This is the “base rate” or how frequent the class is in the training data.
(Represents the initial belief about the probability of each class before observing any evidence.)</p></li>
<li><p><strong>Evidence</strong> (<span class="math notranslate nohighlight">\( p(d) \)</span>):<br />
The probability of observing instance <span class="math notranslate nohighlight">\( d \)</span> across all classes. Acts as a scaling factor to ensure the posterior probabilities sum to 1 and can often be ignored in classification tasks because it is the same for all classes.</p></li>
</ul>
</section>
<section id="simplified-formula-for-classification">
<h3>Simplified Formula for Classification<a class="headerlink" href="#simplified-formula-for-classification" title="Link to this heading">#</a></h3>
<p>In practice, we often ignore the evidence term (<span class="math notranslate nohighlight">\( p(d) \)</span>) because it is the same for all classes. Thus, the formula simplifies to:</p>
<div class="math notranslate nohighlight">
\[
p(c_j | d) \propto p(d | c_j) \, p(c_j)
\]</div>
<p>We calculate this for each class and assign the instance <span class="math notranslate nohighlight">\( d \)</span> to the class with the highest posterior probability.</p>
<p><strong>Summary</strong></p>
<ul class="simple">
<li><p><strong>Prior Probability</strong>: Initial belief about the class distribution.</p></li>
<li><p><strong>Likelihood</strong>: Probability of the evidence given the class.</p></li>
<li><p><strong>Evidence</strong>: Normalizing constant (often ignored).</p></li>
<li><p><strong>Posterior Probability</strong>: Final probability of the class given the evidence.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="example-classifying-by-name">
<h3>Example: Classifying by Name<a class="headerlink" href="#example-classifying-by-name" title="Link to this heading">#</a></h3>
<p>Assume that we have two classes:<br />
<span class="math notranslate nohighlight">\( c_1 = \text{male}, \, \text{and} \, c_2 = \text{female}. \)</span></p>
<p>We have a person whose sex we do not know, say “Drew” or <span class="math notranslate nohighlight">\( d \)</span>.<br />
Classifying Drew as male or female is equivalent to asking which is greater:<br />
<span class="math notranslate nohighlight">\( p(\text{male} \, | \, \text{Drew}) \)</span> or <span class="math notranslate nohighlight">\( p(\text{female} \, | \, \text{Drew}) \)</span>.</p>
<div class="math notranslate nohighlight">
\[
p(\text{male} \, | \, \text{Drew}) = \frac{p(\text{Drew} \, | \, \text{male}) \, p(\text{male})}{p(\text{Drew})}
\]</div>
<div class="math notranslate nohighlight">
\[
p(\text{female} \, | \, \text{Drew}) = \frac{p(\text{Drew} \, | \, \text{female}) \, p(\text{female})}{p(\text{Drew})}
\]</div>
</section>
</section>
<section id="id1">
<h2><img alt="" src="_images/BCW_page_11.png" /><a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<section id="officer-drew-example">
<h3>Officer Drew Example<a class="headerlink" href="#officer-drew-example" title="Link to this heading">#</a></h3>
<p><img alt="" src="_images/BCW_page_12.png" /></p>
<p>We have a small database with names and sex:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Sex</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Drew</p></td>
<td><p>Male</p></td>
</tr>
<tr class="row-odd"><td><p>Claudia</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-even"><td><p>Drew</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-odd"><td><p>Drew</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-even"><td><p>Alberto</p></td>
<td><p>Male</p></td>
</tr>
<tr class="row-odd"><td><p>Karin</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-even"><td><p>Nina</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-odd"><td><p>Sergio</p></td>
<td><p>Male</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="calculating-probabilities-for-officer-drew">
<h3>Calculating Probabilities for Officer Drew<a class="headerlink" href="#calculating-probabilities-for-officer-drew" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
p(c_j | d) = \frac{p(d | c_j) p(c_j)}{p(d)}
\]</div>
<div class="math notranslate nohighlight">
\[
p(\text{male} | \text{Drew}) = \frac{1/3 \times 3/8}{3/8} = \frac{0.125}{3/8}
\]</div>
<div class="math notranslate nohighlight">
\[
p(\text{female} | \text{Drew}) = \frac{2/5 \times 5/8}{3/8} = \frac{0.250}{3/8}
\]</div>
<p>Officer Drew is more likely to be a Female!</p>
<p><img alt="" src="_images/BCW_page_14.png" /></p>
</section>
<hr class="docutils" />
<section id="naive-bayes-with-multiple-features">
<h3>Naive Bayes with Multiple Features<a class="headerlink" href="#naive-bayes-with-multiple-features" title="Link to this heading">#</a></h3>
<p>So far, we have only considered Bayes Classification when we have one attribute (e.g., “antennae length” or “name”). But we may have many features. How do we use all the features?</p>
<div class="math notranslate nohighlight">
\[
p(c_j | d) = \frac{p(d | c_j) p(c_j)}{p(d)}
\]</div>
<p>To simplify the task, <strong>naive Bayesian classifiers</strong> assume attributes have independent distributions, and thereby estimate:</p>
<div class="math notranslate nohighlight">
\[
p(d | c_j) = p(d_1 | c_j) \times p(d_2 | c_j) \times \ldots \times p(d_n | c_j)
\]</div>
</section>
<hr class="docutils" />
<section id="example-officer-drew-with-multiple-features">
<h3>Example: Officer Drew with Multiple Features<a class="headerlink" href="#example-officer-drew-with-multiple-features" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Over 170cm</p></th>
<th class="head"><p>Eye</p></th>
<th class="head"><p>Hair length</p></th>
<th class="head"><p>Sex</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Drew</p></td>
<td><p>No</p></td>
<td><p>Blue</p></td>
<td><p>Short</p></td>
<td><p>Male</p></td>
</tr>
<tr class="row-odd"><td><p>Claudia</p></td>
<td><p>Yes</p></td>
<td><p>Brown</p></td>
<td><p>Long</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-even"><td><p>Drew</p></td>
<td><p>No</p></td>
<td><p>Blue</p></td>
<td><p>Long</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-odd"><td><p>Drew</p></td>
<td><p>No</p></td>
<td><p>Blue</p></td>
<td><p>Long</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-even"><td><p>Alberto</p></td>
<td><p>Yes</p></td>
<td><p>Brown</p></td>
<td><p>Short</p></td>
<td><p>Male</p></td>
</tr>
<tr class="row-odd"><td><p>Karin</p></td>
<td><p>No</p></td>
<td><p>Blue</p></td>
<td><p>Long</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-even"><td><p>Nina</p></td>
<td><p>Yes</p></td>
<td><p>Brown</p></td>
<td><p>Short</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-odd"><td><p>Sergio</p></td>
<td><p>Yes</p></td>
<td><p>Blue</p></td>
<td><p>Long</p></td>
<td><p>Male</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="calculating-probabilities-for-officer-drew-with-multiple-features">
<h3>Calculating Probabilities for Officer Drew with Multiple Features<a class="headerlink" href="#calculating-probabilities-for-officer-drew-with-multiple-features" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
p(\text{Officer Drew} | \text{Female}) = p(\text{Over 170cm} = \text{Yes} | \text{Female}) \times p(\text{Eye} = \text{Blue} | \text{Female}) \times \ldots
\]</div>
<div class="math notranslate nohighlight">
\[
p(\text{Officer Drew} | \text{Male}) = p(\text{Over 170cm} = \text{Yes} | \text{Male}) \times p(\text{Eye} = \text{Blue} | \text{Male}) \times \ldots
\]</div>
<hr class="docutils" />
<p>Both k-NN and NaiveBayes are classification algorithms. Conceptually, k-NN uses the idea of “nearness” to classify new entities. In k-NN ‘nearness’ is modeled with ideas such as Euclidean Distance or Cosine Distance. By contrast, in NaiveBayes, the concept of ‘probability’ is used to classify new entities.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="naive-bayes-classification-with-multiple-features-fruit-example">
<h1>Naive Bayes Classification with multiple features: Fruit Example<a class="headerlink" href="#naive-bayes-classification-with-multiple-features-fruit-example" title="Link to this heading">#</a></h1>
<p>In <a class="reference external" href="https://stackoverflow.com/a/20556654/1013114">this example</a>, we will use Naive Bayes to classify fruits based on their characteristics. We have a dataset of 1000 fruits, which are either <strong>Banana</strong>, <strong>Orange</strong>, or <strong>Other Fruit</strong>. Each fruit has three features:</p>
<ol class="arabic simple">
<li><p><strong>Long</strong>: Whether the fruit is long.</p></li>
<li><p><strong>Sweet</strong>: Whether the fruit is sweet.</p></li>
<li><p><strong>Yellow</strong>: Whether the fruit is yellow.</p></li>
</ol>
<hr class="docutils" />
<section id="training-data">
<h2>Training Data<a class="headerlink" href="#training-data" title="Link to this heading">#</a></h2>
<p>Here is the training data:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Type</p></th>
<th class="head"><p>Long</p></th>
<th class="head"><p>Not Long</p></th>
<th class="head"><p>Sweet</p></th>
<th class="head"><p>Not Sweet</p></th>
<th class="head"><p>Yellow</p></th>
<th class="head"><p>Not Yellow</p></th>
<th class="head"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Banana</p></td>
<td><p>400</p></td>
<td><p>100</p></td>
<td><p>350</p></td>
<td><p>150</p></td>
<td><p>450</p></td>
<td><p>50</p></td>
<td><p>500</p></td>
</tr>
<tr class="row-odd"><td><p>Orange</p></td>
<td><p>0</p></td>
<td><p>300</p></td>
<td><p>150</p></td>
<td><p>150</p></td>
<td><p>300</p></td>
<td><p>0</p></td>
<td><p>300</p></td>
</tr>
<tr class="row-even"><td><p>Other Fruit</p></td>
<td><p>100</p></td>
<td><p>100</p></td>
<td><p>150</p></td>
<td><p>50</p></td>
<td><p>50</p></td>
<td><p>150</p></td>
<td><p>200</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Total</strong></p></td>
<td><p>500</p></td>
<td><p>500</p></td>
<td><p>650</p></td>
<td><p>350</p></td>
<td><p>800</p></td>
<td><p>200</p></td>
<td><p>1000</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="step-1-calculate-prior-probabilities">
<h2>Step 1: Calculate Prior Probabilities<a class="headerlink" href="#step-1-calculate-prior-probabilities" title="Link to this heading">#</a></h2>
<p>The prior probabilities (base rates) are calculated based on the total number of fruits in each class:</p>
<div class="math notranslate nohighlight">
\[
P(\text{Banana}) = \frac{500}{1000} = 0.5
\]</div>
<div class="math notranslate nohighlight">
\[
P(\text{Orange}) = \frac{300}{1000} = 0.3
\]</div>
<div class="math notranslate nohighlight">
\[
P(\text{Other Fruit}) = \frac{200}{1000} = 0.2
\]</div>
</section>
<hr class="docutils" />
<section id="step-2-calculate-evidence-probabilities">
<h2>Step 2: Calculate Evidence Probabilities<a class="headerlink" href="#step-2-calculate-evidence-probabilities" title="Link to this heading">#</a></h2>
<p>The probabilities of each feature (evidence) in the entire dataset are:</p>
<div class="math notranslate nohighlight">
\[
P(\text{Long}) = \frac{500}{1000} = 0.5
\]</div>
<div class="math notranslate nohighlight">
\[
P(\text{Sweet}) = \frac{650}{1000} = 0.65
\]</div>
<div class="math notranslate nohighlight">
\[
P(\text{Yellow}) = \frac{800}{1000} = 0.8
\]</div>
</section>
<hr class="docutils" />
<section id="step-3-calculate-likelihoods">
<h2>Step 3: Calculate Likelihoods<a class="headerlink" href="#step-3-calculate-likelihoods" title="Link to this heading">#</a></h2>
<p>Next, we calculate the likelihood of each feature given the class.</p>
<section id="for-banana">
<h3>For <strong>Banana</strong>:<a class="headerlink" href="#for-banana" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
P(\text{Long} | \text{Banana}) = \frac{400}{500} = 0.8
\]</div>
<div class="math notranslate nohighlight">
\[
P(\text{Sweet} | \text{Banana}) = \frac{350}{500} = 0.7
\]</div>
<div class="math notranslate nohighlight">
\[
P(\text{Yellow} | \text{Banana}) = \frac{450}{500} = 0.9
\]</div>
</section>
<hr class="docutils" />
<section id="for-orange">
<h3>For <strong>Orange</strong>:<a class="headerlink" href="#for-orange" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
P(\text{Long} | \text{Orange}) = \frac{0}{300} = 0
\]</div>
<div class="math notranslate nohighlight">
\[
P(\text{Sweet} | \text{Orange}) = \frac{150}{300} = 0.5
\]</div>
<div class="math notranslate nohighlight">
\[
P(\text{Yellow} | \text{Orange}) = \frac{300}{300} = 1
\]</div>
</section>
<hr class="docutils" />
<section id="for-other-fruit">
<h3>For <strong>Other Fruit</strong>:<a class="headerlink" href="#for-other-fruit" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
P(\text{Long} | \text{Other Fruit}) = \frac{100}{200} = 0.5
\]</div>
<div class="math notranslate nohighlight">
\[
P(\text{Sweet} | \text{Other Fruit}) = \frac{150}{200} = 0.75
\]</div>
<div class="math notranslate nohighlight">
\[
P(\text{Yellow} | \text{Other Fruit}) = \frac{50}{200} = 0.25
\]</div>
</section>
</section>
<hr class="docutils" />
<section id="step-4-classify-a-new-fruit">
<h2>Step 4: Classify a New Fruit<a class="headerlink" href="#step-4-classify-a-new-fruit" title="Link to this heading">#</a></h2>
<p>Suppose we have a new fruit with the following features:</p>
<ul class="simple">
<li><p><strong>Long</strong>: Yes</p></li>
<li><p><strong>Sweet</strong>: Yes</p></li>
<li><p><strong>Yellow</strong>: Yes</p></li>
</ul>
<p>We want to classify this fruit as either a <strong>Banana</strong>, <strong>Orange</strong>, or <strong>Other Fruit</strong>.</p>
<hr class="docutils" />
<section id="calculate-posterior-probabilities">
<h3>Calculate Posterior Probabilities<a class="headerlink" href="#calculate-posterior-probabilities" title="Link to this heading">#</a></h3>
<p>We use Bayes’ Theorem to calculate the posterior probability for each class:</p>
<div class="math notranslate nohighlight">
\[
P(\text{Class} | \text{Evidence}) = \frac{P(\text{Evidence} | \text{Class}) \times P(\text{Class})}{P(\text{Evidence})}
\]</div>
<p>Since Naive Bayes assumes independence between features, we can simplify this to:</p>
<div class="math notranslate nohighlight">
\[
P(\text{Class} | \text{Evidence}) = P(\text{Long} | \text{Class}) \times P(\text{Sweet} | \text{Class}) \times P(\text{Yellow} | \text{Class}) \times P(\text{Class})
\]</div>
<hr class="docutils" />
<section id="id2">
<h4>For <strong>Banana</strong>:<a class="headerlink" href="#id2" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
P(\text{Banana} | \text{Long, Sweet, Yellow}) = P(\text{Long} | \text{Banana}) \times P(\text{Sweet} | \text{Banana}) \times P(\text{Yellow} | \text{Banana}) \times P(\text{Banana})
\]</div>
<div class="math notranslate nohighlight">
\[
P(\text{Banana} | \text{Long, Sweet, Yellow}) = 0.8 \times 0.7 \times 0.9 \times 0.5 = 0.252
\]</div>
</section>
<hr class="docutils" />
<section id="id3">
<h4>For <strong>Orange</strong>:<a class="headerlink" href="#id3" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
P(\text{Orange} | \text{Long, Sweet, Yellow}) = P(\text{Long} | \text{Orange}) \times P(\text{Sweet} | \text{Orange}) \times P(\text{Yellow} | \text{Orange}) \times P(\text{Orange})
\]</div>
<div class="math notranslate nohighlight">
\[
P(\text{Orange} | \text{Long, Sweet, Yellow}) = 0 \times 0.5 \times 1 \times 0.3 = 0
\]</div>
</section>
<hr class="docutils" />
<section id="id4">
<h4>For <strong>Other Fruit</strong>:<a class="headerlink" href="#id4" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
P(\text{Other Fruit} | \text{Long, Sweet, Yellow}) = P(\text{Long} | \text{Other Fruit}) \times P(\text{Sweet} | \text{Other Fruit}) \times P(\text{Yellow} | \text{Other Fruit}) \times P(\text{Other Fruit})
\]</div>
<div class="math notranslate nohighlight">
\[
P(\text{Other Fruit} | \text{Long, Sweet, Yellow}) = 0.5 \times 0.75 \times 0.25 \times 0.2 = 0.01875
\]</div>
</section>
</section>
<hr class="docutils" />
<section id="step-5-make-a-prediction">
<h3>Step 5: Make a Prediction<a class="headerlink" href="#step-5-make-a-prediction" title="Link to this heading">#</a></h3>
<p>The posterior probabilities are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( P(\text{Banana} | \text{Long, Sweet, Yellow}) = 0.252 \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( P(\text{Orange} | \text{Long, Sweet, Yellow}) = 0 \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( P(\text{Other Fruit} | \text{Long, Sweet, Yellow}) = 0.01875 \)</span></p></li>
</ul>
<p>Since <span class="math notranslate nohighlight">\( P(\text{Banana} | \text{Long, Sweet, Yellow}) \)</span> is the highest, we classify the new fruit as a <strong>Banana</strong>.</p>
</section>
</section>
<hr class="docutils" />
<section id="key-takeaways">
<h2>Key Takeaways<a class="headerlink" href="#key-takeaways" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Naive Bayes is a simple and efficient classification algorithm.</p></li>
<li><p>It assumes that features are independent of each other.</p></li>
<li><p>The algorithm works by calculating the posterior probability for each class and selecting the class with the highest probability.</p></li>
<li><p>Despite its simplicity, Naive Bayes performs well in many real-world applications, such as text classification.</p></li>
</ul>
<hr class="docutils" />
<p>This example demonstrates how Naive Bayes can be used to classify fruits based on multiple features. The same approach can be applied to other classification problems.</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-whitegrid&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="gaussian-naive-bayes">
<h2>Gaussian Naive Bayes<a class="headerlink" href="#gaussian-naive-bayes" title="Link to this heading">#</a></h2>
<p>Perhaps the easiest naive Bayes classifier to understand is Gaussian naive Bayes.
With this classifier, the assumption is that <em>data from each label is drawn from a simple Gaussian distribution</em>.
Imagine that we have the following data, shown in Figure 41-1:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_blobs</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7c507512dde93938140623005ab9d482e894b70d9bd7ac448300e7eea500b645.png" src="_images/7c507512dde93938140623005ab9d482e894b70d9bd7ac448300e7eea500b645.png" />
</div>
</div>
<p>The simplest Gaussian model is to assume that the data is described by a Gaussian distribution with no covariance between dimensions.
This model can be fit by computing the mean and standard deviation of the points within each label, which is all we need to define such a distribution.
The result of this naive Gaussian assumption is shown in the following figure:</p>
<p><img alt="" src="_images/05.05-gaussian-NB.png" /></p>
<p>The ellipses here represent the Gaussian generative model for each label, with larger probability toward the center of the ellipses.
With this generative model in place for each class, we have a simple recipe to compute the likelihood <span class="math notranslate nohighlight">\(P({\rm features}~|~L_1)\)</span> for any data point, and thus we can quickly compute the posterior ratio and determine which label is the most probable for a given point.</p>
<p>This procedure is implemented in Scikit-Learn’s <code class="docutils literal notranslate"><span class="pre">sklearn.naive_bayes.GaussianNB</span></code> estimator:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.naive_bayes</span><span class="w"> </span><span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s generate some new data and predict the label:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">Xnew</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">14</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">14</span><span class="p">,</span> <span class="mi">18</span><span class="p">]</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ynew</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can plot this new data to get an idea of where the decision boundary is (see the following figure):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu&#39;</span><span class="p">)</span>
<span class="n">lim</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Xnew</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Xnew</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">ynew</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="n">lim</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e6e81beb929ab0d719a0162e41139fd606711367d13ab5a2f68159695093ec52.png" src="_images/e6e81beb929ab0d719a0162e41139fd606711367d13ab5a2f68159695093ec52.png" />
</div>
</div>
<p>We see a slightly curved boundary in the classifications—in general, the boundary produced by a Gaussian naive Bayes model will be quadratic.</p>
<p>A nice aspect of this Bayesian formalism is that it naturally allows for probabilistic classification, which we can compute using the <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yprob</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>
<span class="n">yprob</span><span class="p">[</span><span class="o">-</span><span class="mi">8</span><span class="p">:]</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.89, 0.11],
       [1.  , 0.  ],
       [1.  , 0.  ],
       [1.  , 0.  ],
       [1.  , 0.  ],
       [1.  , 0.  ],
       [0.  , 1.  ],
       [0.15, 0.85]])
</pre></div>
</div>
</div>
</div>
<p>The columns give the posterior probabilities of the first and second labels, respectively.
If you are looking for estimates of uncertainty in your classification, Bayesian approaches like this can be a good place to start.</p>
<p>Of course, the final classification will only be as good as the model assumptions that lead to it, which is why Gaussian naive Bayes often does not produce very good results.
Still, in many cases—especially as the number of features becomes large—this assumption is not detrimental enough to prevent Gaussian naive Bayes from being a reliable method.</p>
</section>
<section id="multinomial-naive-bayes">
<h2>Multinomial Naive Bayes<a class="headerlink" href="#multinomial-naive-bayes" title="Link to this heading">#</a></h2>
<p>The Gaussian assumption just described is by no means the only simple assumption that could be used to specify the generative distribution for each label.
Another useful example is multinomial naive Bayes, where the features are assumed to be generated from a simple multinomial distribution.
The multinomial distribution describes the probability of observing counts among a number of categories, and thus multinomial naive Bayes is most appropriate for features that represent counts or count rates.</p>
<p>The idea is precisely the same as before, except that instead of modeling the data distribution with the best-fit Gaussian, we model it with a best-fit multinomial distribution.</p>
<section id="example-classifying-text">
<h3>Example: Classifying Text<a class="headerlink" href="#example-classifying-text" title="Link to this heading">#</a></h3>
<p>One place where multinomial naive Bayes is often used is in text classification, where the features are related to word counts or frequencies within the documents to be classified.
We discussed the extraction of such features from text in <a class="reference internal" href="#05.04-Feature-Engineering.ipynb"><span class="xref myst">Feature Engineering</span></a>; here we will use the sparse word count features from the 20 Newsgroups corpus made available through Scikit-Learn to show how we might classify these short documents into categories.</p>
<p>Let’s download the data and take a look at the target names:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">()</span>
<span class="n">data</span><span class="o">.</span><span class="n">target_names</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;alt.atheism&#39;,
 &#39;comp.graphics&#39;,
 &#39;comp.os.ms-windows.misc&#39;,
 &#39;comp.sys.ibm.pc.hardware&#39;,
 &#39;comp.sys.mac.hardware&#39;,
 &#39;comp.windows.x&#39;,
 &#39;misc.forsale&#39;,
 &#39;rec.autos&#39;,
 &#39;rec.motorcycles&#39;,
 &#39;rec.sport.baseball&#39;,
 &#39;rec.sport.hockey&#39;,
 &#39;sci.crypt&#39;,
 &#39;sci.electronics&#39;,
 &#39;sci.med&#39;,
 &#39;sci.space&#39;,
 &#39;soc.religion.christian&#39;,
 &#39;talk.politics.guns&#39;,
 &#39;talk.politics.mideast&#39;,
 &#39;talk.politics.misc&#39;,
 &#39;talk.religion.misc&#39;]
</pre></div>
</div>
</div>
</div>
<p>For simplicity here, we will select just a few of these categories and download the training and testing sets:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">categories</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;talk.religion.misc&#39;</span><span class="p">,</span> <span class="s1">&#39;soc.religion.christian&#39;</span><span class="p">,</span>
              <span class="s1">&#39;sci.space&#39;</span><span class="p">,</span> <span class="s1">&#39;comp.graphics&#39;</span><span class="p">]</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">)</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here is a representative entry from the data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">5</span><span class="p">][</span><span class="mi">48</span><span class="p">:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Subject: Federal Hearing
Originator: dmcgee@uluhe
Organization: School of Ocean and Earth Science and Technology
Distribution: usa
Lines: 10


Fact or rumor....?  Madalyn Murray O&#39;Hare an atheist who eliminated the
use of the bible reading and prayer in public schools 15 years ago is now
going to appear before the FCC with a petition to stop the reading of the
Gospel on the airways of America.  And she is also campaigning to remove
Christmas programs, songs, etc from the public schools.  If it is true
then mail to Federal Communications Commission 1919 H Street Washington DC
20054 expressing your opposition to her request.  Reference Petition number

2493.
</pre></div>
</div>
</div>
</div>
<p>In order to use this data for machine learning, we need to be able to convert the content of each string into a vector of numbers.
For this we will use the TF-IDF vectorizer (introduced in <a class="reference internal" href="#05.04-Feature-Engineering.ipynb"><span class="xref myst">Feature Engineering</span></a>), and create a pipeline that attaches it to a multinomial naive Bayes classifier:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_extraction.text</span><span class="w"> </span><span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.naive_bayes</span><span class="w"> </span><span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">TfidfVectorizer</span><span class="p">(),</span> <span class="n">MultinomialNB</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<p>With this pipeline, we can apply the model to the training data and predict labels for the test data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">train</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have predicted the labels for the test data, we can evaluate them to learn about the performance of the estimator.
For example, let’s take a look at the confusion matrix between the true and predicted labels for the test data (see the following figure):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="n">mat</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">test</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">xticklabels</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">target_names</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">target_names</span><span class="p">,</span>
            <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;true label&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;predicted label&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/77008674613cf1ec11cabc493301a00cd7309d2442ae7af337b02876faeb5e31.png" src="_images/77008674613cf1ec11cabc493301a00cd7309d2442ae7af337b02876faeb5e31.png" />
</div>
</div>
<p>Evidently, even this very simple classifier can successfully separate space discussions from computer discussions, but it gets confused between discussions about religion and discussions about Christianity.
This is perhaps to be expected!</p>
<p>The cool thing here is that we now have the tools to determine the category for <em>any</em> string, using the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method of this pipeline.
Here’s a utility function that will return the prediction for a single string:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">predict_category</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">):</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">s</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">train</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s try it out:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predict_category</span><span class="p">(</span><span class="s1">&#39;sending a payload to the ISS&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;sci.space&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predict_category</span><span class="p">(</span><span class="s1">&#39;discussing the existence of God&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;soc.religion.christian&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predict_category</span><span class="p">(</span><span class="s1">&#39;determining the screen resolution&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;comp.graphics&#39;
</pre></div>
</div>
</div>
</div>
<p>Remember that this is nothing more sophisticated than a simple probability model for the (weighted) frequency of each word in the string; nevertheless, the result is striking.
Even a very naive algorithm, when used carefully and trained on a large set of high-dimensional data, can be surprisingly effective.</p>
</section>
</section>
<section id="further-readning">
<h2>Further Readning<a class="headerlink" href="#further-readning" title="Link to this heading">#</a></h2>
<section id="when-to-use-naive-bayes">
<h3>When to Use Naive Bayes<a class="headerlink" href="#when-to-use-naive-bayes" title="Link to this heading">#</a></h3>
<p>Because naive Bayes classifiers make such stringent assumptions about data, they will generally not perform as well as more complicated models.
That said, they have several advantages:</p>
<ul class="simple">
<li><p>They are fast for both training and prediction.</p></li>
<li><p>They provide straightforward probabilistic prediction.</p></li>
<li><p>They are often easily interpretable.</p></li>
<li><p>They have few (if any) tunable parameters.</p></li>
</ul>
<p>These advantages mean a naive Bayes classifier is often a good choice as an initial baseline classification.
If it performs suitably, then congratulations: you have a very fast, very interpretable classifier for your problem.
If it does not perform well, then you can begin exploring more sophisticated models, with some baseline knowledge of how well they should perform.</p>
<p>Naive Bayes classifiers tend to perform especially well in the following situations:</p>
<ul class="simple">
<li><p>When the naive assumptions actually match the data (very rare in practice)</p></li>
<li><p>For very well-separated categories, when model complexity is less important</p></li>
<li><p>For very high-dimensional data, when model complexity is less important</p></li>
</ul>
<p>The last two points seem distinct, but they actually are related: as the dimensionality of a dataset grows, it is much less likely for any two points to be found close together (after all, they must be close in <em>every single dimension</em> to be close overall).
This means that clusters in high dimensions tend to be more separated, on average, than clusters in low dimensions, assuming the new dimensions actually add information.
For this reason, simplistic classifiers like the ones discussed here tend to work as well or better than more complicated classifiers as the dimensionality grows: once you have enough data, even a simple model can be very powerful.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="01%20-%20Introduction.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Introduction</p>
      </div>
    </a>
    <a class="right-next"
       href="Bayesian-Decision-Theory.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Bayesian Decision Theory</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Naive Bayes Classification</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes-classifier">Naive Bayes Classifier</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#grasshoppers-vs-katydids">Grasshoppers vs. Katydids</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#antenna-length-vs-abdomen-length">Antenna Length vs. Abdomen Length</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classifying-an-insect">Classifying an Insect</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-antennae-length-is-3">Example: Antennae Length is 3</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-classifiers">Bayes Classifiers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-classifiers">Bayesian Classifiers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-classifiers-and-bayes-theorem">Bayesian Classifiers and Bayes’ Theorem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simplified-formula-for-classification">Simplified Formula for Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-classifying-by-name">Example: Classifying by Name</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#officer-drew-example">Officer Drew Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-probabilities-for-officer-drew">Calculating Probabilities for Officer Drew</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes-with-multiple-features">Naive Bayes with Multiple Features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-officer-drew-with-multiple-features">Example: Officer Drew with Multiple Features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-probabilities-for-officer-drew-with-multiple-features">Calculating Probabilities for Officer Drew with Multiple Features</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes-classification-with-multiple-features-fruit-example">Naive Bayes Classification with multiple features: Fruit Example</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-data">Training Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-calculate-prior-probabilities">Step 1: Calculate Prior Probabilities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-calculate-evidence-probabilities">Step 2: Calculate Evidence Probabilities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-calculate-likelihoods">Step 3: Calculate Likelihoods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#for-banana">For <strong>Banana</strong>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#for-orange">For <strong>Orange</strong>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#for-other-fruit">For <strong>Other Fruit</strong>:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-classify-a-new-fruit">Step 4: Classify a New Fruit</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculate-posterior-probabilities">Calculate Posterior Probabilities</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">For <strong>Banana</strong>:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">For <strong>Orange</strong>:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">For <strong>Other Fruit</strong>:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-make-a-prediction">Step 5: Make a Prediction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-takeaways">Key Takeaways</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-naive-bayes">Gaussian Naive Bayes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-naive-bayes">Multinomial Naive Bayes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-classifying-text">Example: Classifying Text</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-readning">Further Readning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-naive-bayes">When to Use Naive Bayes</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Mahmood Amintoosi
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024. CC0 Licensed - Computer Science Dept., Ferdowsi University of Mashhad.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>